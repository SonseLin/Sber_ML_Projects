{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive an analytical solution to the regression problem. Use a vector form of the equation.\n",
    "\n",
    "Для начала векторная форма регрессии - y = xW + b\n",
    "где xW это (w0x1 + w1x2 + ... wNxN)\n",
    "b - вектор ошибок т е смещение\n",
    "\n",
    "мы хотим минимизировать сумму квадратов ошибок SSE = | y - xW | ^ 2 = (y - xW)^t(y - xW)\n",
    "при раскрытии получаем SSE = y^t * y - 2 x^t W^t y + x^t W^t xW\n",
    "\n",
    "∂SSE / ∂T = -2 x^t y + 2 xW x^t = 0\n",
    "2 x^t xW = 2 x^t y\n",
    "x^t xW = x^t y\n",
    "\n",
    "если матрица x^t обратима, можно домножить обе части на обратную (x^t x)^-1\n",
    "\n",
    "W = (x^t x)^-1 x^t y,\n",
    "где x^t x - матрица взаимосвязей \n",
    "x^t y - вектор взаимосвязи между признаками и таргетом\n",
    "\n",
    "##### Зачем брали производную?\n",
    "\n",
    "В матане для нахождения минимума функции(в нашем случае функции потерь), нужно взять производную функции и приравнять к 0\n",
    "\n",
    "##### Почему именно по W?\n",
    "\n",
    "Потому что именно их и оптимизируем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What changes in the solution when L1 and L2 regularizations are added to the loss function. \n",
    "\n",
    "l2 регуляция ∑(l...d) w(i...2) уменьшает коэффициенты весов и и добавленная к обратимой матрице коэффициенты регуляция обеспечивает матрицу обратимостью"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l1 регуляция ∑(l...d) |wi| увеличивает разряженность коэффициентов, некоторые становятся равны 0, и происходит перераспределение весов фич(зачем? пока хз)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain why L1 regularization is often used to select features. Why are there many weights equal to 0 after the model is fit?\n",
    "\n",
    "Зачем? когда имеем очень много фич и нужно вычленить самые важные. Если изменение веса определенной фичи имеет очень малый импакт(малое изменение предиката), то обрасываем параметр за неважностью, устанавливая вес в 0\n",
    "\n",
    "Почему используем? Отбрасывая фичи, мы упрощаем нашу модель и делаем более легкое для интерпретации, также избавляет нас от мануального перебора фич"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain how you can use the same models (Linear regression, Ridge, etc.) but make it possible to fit nonlinear dependencies.\n",
    "\n",
    "Можно использовать фичер инжиниринг или кернел методы, позволяющие нам преобразовывать входные данные или саму модель для улавливания нелинейных взаимосвязей, сохраняя при этом линейные модели в их основе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
